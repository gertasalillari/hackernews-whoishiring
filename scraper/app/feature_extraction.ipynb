{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbd1c4bb-decb-4234-85d9-6c7bf6c236de",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "After extracting the comments from the Hacker News posts, this notebook processes the data (headline and body), and attempts to extract relevant information:\n",
    "\n",
    "* select only early-stage companies (exclude companies that have gone through Series A, B, C or D)\n",
    "* company name\n",
    "* location (using SpaCy and Flair, then data from GeoNames is used to determine if the location is in Europe or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00bdc9e4-39cd-4677-a77a-2c939ae416c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "\n",
    "import tldextract\n",
    "from Levenshtein import ratio\n",
    "\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b90a34c-2c13-4e84-82a3-94597133b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "loaded_data = []\n",
    "with open(\"../data/hacker_news_comments.jsonl\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        json_object = json.loads(line)\n",
    "        loaded_data.append(json_object)\n",
    "\n",
    "df = pd.DataFrame(loaded_data)\n",
    "\n",
    "df[\"company_name\"] = df['headline'].apply(lambda x: \"|\".join(x.split(\"|\")[:1]).strip())\n",
    "df[\"location\"] = df['headline'].apply(lambda x: \"|\".join(x.split(\"|\")[1:2]).strip().lower())\n",
    "df[\"post_info\"] = df['headline'].apply(lambda x: \"|\".join(x.split(\"|\")[2:]).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc475877-e32e-4df5-8527-e5b2e8d06e10",
   "metadata": {},
   "source": [
    "## Early-stage companies\n",
    "Select only early-stage companies, by excluding companies that have already gone through a series A, B, C or D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1e9503-644e-423e-aae6-92dcf7ae25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_early_stage_companies(df):\n",
    "    series_pattern = r\"series\\s+[abcd]\"\n",
    "    df = df[\n",
    "        (~df[\"headline\"].str.contains(series_pattern, case=False, na=False))\n",
    "        & (~df[\"body\"].str.contains(series_pattern, case=False, na=False))\n",
    "    ]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7bac631-56bf-48d9-aed2-8c1ba31cddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filter_early_stage_companies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5996609c-74f3-4fc1-a40d-dc5a359ea1c9",
   "metadata": {},
   "source": [
    "## Location Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65804d6-e17d-4fea-99be-a1b697331354",
   "metadata": {},
   "source": [
    "#### Geonames\n",
    "Use Geonames to determine if a found location (using SpaCy) is in Europe or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67f9bb44-94d7-4f51-b6a7-1ce96e93bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_geonames_loacations(filepath):\n",
    "    geonames_df = pd.read_csv(\n",
    "        filepath, delimiter=\";\", na_values=None, keep_default_na=False\n",
    "    )\n",
    "\n",
    "    geonames_df = geonames_df[geonames_df[\"Population\"] >= 120000]\n",
    "    geonames_df = geonames_df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    # Remove \"city\" from the city names. Eg: New York city -> New York\n",
    "    geonames_df[\"ASCII Name\"] = geonames_df[\"ASCII Name\"].str.replace(\" city\", \"\")\n",
    "    geonames_df[\"Country name EN\"] = geonames_df[\"Country name EN\"].apply(\n",
    "        lambda x: x.split(\",\")[0].strip()\n",
    "    )\n",
    "    # For cities with the same name, keep only the biggest (population-wise)\n",
    "    geonames_df_sorted = geonames_df.sort_values(\n",
    "        by=[\"Name\", \"Population\"], ascending=[True, False]\n",
    "    )\n",
    "    geonames_df = geonames_df_sorted.groupby(\"Name\").first().reset_index()\n",
    "\n",
    "    # Filter European locations\n",
    "    eu_locations = geonames_df[geonames_df[\"Timezone\"].str.startswith(\"europe\")][\n",
    "        [\"ASCII Name\", \"Country name EN\"]\n",
    "    ].values\n",
    "    eu_locations = set(element for sublist in eu_locations for element in sublist)\n",
    "\n",
    "    # Filter USA locations\n",
    "    usa_locations = set(\n",
    "        geonames_df[geonames_df[\"Country name EN\"] == \"united states\"][\"Name\"]\n",
    "    )\n",
    "\n",
    "    # Filter all other locations\n",
    "    other_locations = geonames_df[\n",
    "        (~geonames_df[\"Timezone\"].str.startswith(\"europe\"))\n",
    "        & (geonames_df[\"Country name EN\"] != \"united states\")\n",
    "    ][[\"ASCII Name\", \"Country name EN\"]].values\n",
    "    other_locations = set(element for sublist in other_locations for element in sublist)\n",
    "\n",
    "    return eu_locations, usa_locations, other_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "068888e2-b750-44ce-a359-53eac61a86f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Geonames data\n",
    "eu_locations, usa_locations, other_locations = load_geonames_loacations(\"../data/geonames_1000.csv\")\n",
    "\n",
    "eu_locations.update([\"europe\", \"european\", \"emea\", \"â‚¬\", \"Â£\"])\n",
    "usa_locations.update([\"united states\", \"bay area\", \"palo alto\", \"us remote\", \"america\", \"nyc\", \"usd\", \"$\", \"new york\", \"usa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d31f06-0676-4696-b95e-3dbee51cc677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_continent(obj):\n",
    "    continent = \"unknown\"\n",
    "\n",
    "    if obj in eu_locations:\n",
    "        continent = \"europe\"\n",
    "    if obj in usa_locations:\n",
    "        continent = \"usa\"\n",
    "    elif obj in other_locations:\n",
    "        continent = \"other\"\n",
    "\n",
    "    return continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffa0b4e1-916b-4c27-996f-b4bf069ac55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic string matching\n",
    "df[\"region\"] = df[\"location\"].apply(assign_continent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7611c8f0-6923-4e18-9402-e400bcfce793",
   "metadata": {},
   "source": [
    "#### Name Entity Recognition with SpaCy\n",
    "Try NER on the columns `location`, `post_info` and `body`, and assign the location to a continent using the GeoNames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a480c410-e266-460a-a0b4-58f861533298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model\n",
    "spacy_model = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b99e8ef-f305-45cb-b5af-1e171c3104e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to find location first in the location column, then in the post_info, and lastly in the post's body\n",
    "def ner_location(df, nlp):\n",
    "    locations = []\n",
    "    for column_value in df:\n",
    "        doc = nlp(column_value)\n",
    "        locations = [ent.text for ent in doc.ents if ent.label_ == \"GPE\"]\n",
    "        # found location\n",
    "        if len(locations) > 0:\n",
    "            break\n",
    "    return list(map(lambda x: x.lower(), set(locations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c33c67d-af93-481e-83f9-bfddcb5c57c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a423d97c28b94be2b07fecb2755ac356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Search location only for instances without a EU/US tag\n",
    "no_location_index = df[(df[\"region\"] == \"unknown\")].index\n",
    "\n",
    "for i, row in tqdm(df.loc[no_location_index, [\"location\", \"post_info\", \"body\"]].iterrows(), total=len(no_location_index)):\n",
    "    ner_location_list = ner_location(row, spacy_model)\n",
    "    if ner_location_list:\n",
    "        for location in ner_location_list:\n",
    "            continent = assign_continent(location)\n",
    "            if continent != \"unknown\":\n",
    "                break\n",
    "    df.loc[i, \"region\"] = continent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba534b6e-483c-45ad-81d2-57ea6715bda1",
   "metadata": {},
   "source": [
    "## Company Name Extraction\n",
    "Extracts company names from a DataFrame using various methods:\n",
    "\n",
    "    1. Preproces of the extracted company name (from Headline)\n",
    "    2. Predicting names using NER models (Flair and spaCy).\n",
    "    3. Reconciling predictions and choosing the best candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbb3d3c1-a629-45ac-8ef4-32af874633ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleans a company name string by removing extraneous text and formatting.\n",
    "# Eg: removes text between parenthesis, removes text after \"https\" and after \" - \"\n",
    "\n",
    "def clean_company_name(input_string):\n",
    "    cleaned_string = re.sub(r\"\\(.*?\\).*\", \"\", input_string)\n",
    "    # Split the text at \" https\" (with a space before)\n",
    "    parts = cleaned_string.split(\" https\")\n",
    "\n",
    "    # Clean each part separately\n",
    "    cleaned_parts = []\n",
    "    for part in parts:\n",
    "        # Find the first non-escaped slash\n",
    "        first_unescaped_slash = re.search(r\"(?<!\\\\)/\", part)  # Corrected regex\n",
    "\n",
    "        # Remove everything after the first slash if not preceded by \"https\"\n",
    "        if not part.startswith(\"https\") and first_unescaped_slash:\n",
    "            # Get the substring before the first non-escaped slash\n",
    "            part = part[: first_unescaped_slash.start()]\n",
    "\n",
    "        # Remove everything after \" https\" (with a space before)\n",
    "        part = re.sub(r\" https.*$\", \"\", part)\n",
    "\n",
    "        # Remove everything after \" - \" (with spaces and the hyphen)\n",
    "        part = re.sub(r\"(^|\\s)+\\-+[\\>\\s]+.*\", \"\", part)\n",
    "\n",
    "        # Remove trailing punctuation and leading/trailing whitespace\n",
    "        cleaned_part = part.rstrip(punctuation).strip()\n",
    "\n",
    "        # Add the cleaned part to the list\n",
    "        cleaned_parts.append(cleaned_part)\n",
    "\n",
    "    # Join the cleaned parts back together\n",
    "    return \" \".join(cleaned_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cc9db63-dd77-4209-829f-8f0176e7fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_company_name\"] = df[\"company_name\"].apply(clean_company_name)\n",
    "df.drop([\"company_name\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ebedb4f-267b-49ce-93f5-69fa3ddbef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_company_name(company_names_list, ner_model):\n",
    "    predicted_names = []\n",
    "    for name in tqdm(company_names_list):\n",
    "        if name:\n",
    "            # SpaCy model\n",
    "            if isinstance(ner_model, spacy.lang.en.English):\n",
    "                doc = ner_model(name)\n",
    "                entities = doc.ents\n",
    "            # Flair model\n",
    "            elif isinstance(ner_model, SequenceTagger):\n",
    "                sentence = Sentence(name)\n",
    "                ner_model.predict(sentence)\n",
    "                entities = sentence.get_spans(\"ner\")\n",
    "\n",
    "            if entities:\n",
    "                # If multiple spans found, return the first (higher likelihood)\n",
    "                if len(entities) > 1:\n",
    "                    predicted_names.append(entities[0].text)\n",
    "                else:\n",
    "                    for token in entities:\n",
    "                        predicted_names.append(token.text)\n",
    "            else:\n",
    "                predicted_names.append(\"\")\n",
    "        else:\n",
    "            predicted_names.append(\"\")\n",
    "    return [x.strip() for x in predicted_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7160f668-8220-4f7a-8cb6-e2ccb3c36de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-14 18:38:47,816 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "# Load the spaCy model and the Flair SequenceTagger\n",
    "flair_model = SequenceTagger.load(\"ner-fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d298da90-8615-41b1-a4c1-2f29cb84688d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef44399170ab43f4a9865c28b3b651e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80f871deaee4cb883ef86e2bc4731c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names2predict = df[\"cleaned_company_name\"].tolist()\n",
    "\n",
    "flair_ner = \"flair_company_name\"\n",
    "spacy_ner = \"spacy_company_name\"\n",
    "\n",
    "# Predict company names with Flair\n",
    "df[flair_ner] = predict_company_name(names2predict, flair_model)\n",
    "# Predict company names with spaCy\n",
    "df[spacy_ner] = predict_company_name(names2predict, spacy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33ba16ee-6eeb-4b5e-85f2-f4283b2663d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconcile company names if different\n",
    "\n",
    "# Fill in missing values from the ner column with ner2 values\n",
    "empty_ner1_index = df[(df[flair_ner] != df[spacy_ner]) & (df[flair_ner] == \"\")].index\n",
    "df.loc[empty_ner1_index, flair_ner] = (\n",
    "    df.loc[empty_ner1_index, flair_ner].replace(\"\", pd.NA).fillna(df.loc[empty_ner1_index, spacy_ner])\n",
    ")\n",
    "\n",
    "# If contrasting entities found, chose the longest string\n",
    "contrasting_ner_index = df[(df[flair_ner] != df[spacy_ner]) & (df[spacy_ner] != \"\")].index\n",
    "df.loc[contrasting_ner_index, flair_ner] = df.loc[contrasting_ner_index, [flair_ner, spacy_ner]].apply(\n",
    "    lambda x: max(x, key=len), axis=1\n",
    ")\n",
    "\n",
    "# If no NER was found, fill with cleaned company name\n",
    "no_ner_index = df[(df[flair_ner] == \"\")].index\n",
    "df.loc[no_ner_index, flair_ner] = (\n",
    "    df.loc[no_ner_index, flair_ner].replace(\"\", pd.NA).fillna(df.loc[no_ner_index, \"cleaned_company_name\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cca6bc6-2e51-4581-a18a-350796bb800b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'flair_company_name': 'company_name'}, inplace=True)\n",
    "df.drop([spacy_ner, \"cleaned_company_name\", \"location\", \"post_info\", \"hash\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a595f17d-2dd5-4da6-b094-d1b916b0b539",
   "metadata": {},
   "source": [
    "## URL Extraction\n",
    "Extracts a company URL from text based on domain similarity to the company name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e5dcb0e-84ec-41a1-84f0-936291e05c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company_url(company_name, text):\n",
    "    regex = r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\"\n",
    "    matches = re.findall(regex, text)\n",
    "    urls = set()\n",
    "    for match in matches:\n",
    "        url = tldextract.extract(match)\n",
    "        urls.add((url.domain, url.suffix))\n",
    "\n",
    "    ratios = [ratio(company_name, domain[0]) for domain in urls]\n",
    "    if ratios:\n",
    "        largest_ratio = max(zip(urls, ratios), key=lambda x: x[1])\n",
    "        largest_ratio_domain = \".\".join(largest_ratio[0])\n",
    "\n",
    "        if largest_ratio[1] > 0.25:\n",
    "            largest_ratio_domain = \".\".join(largest_ratio[0])\n",
    "            return largest_ratio_domain\n",
    "        else:\n",
    "            return \"\"\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14bfc99d-e28a-4546-8f70-0fb65cbe44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"company_url\"] = df[[\"company_name\", \"headline\", 'body']].apply(lambda x: extract_company_url(x[0], x[1]+x[2]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b6ba3bc-66fd-4efa-84e6-c49c0d4c1d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = os.path.join(\"../data/outputs/output.csv\")\n",
    "df.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3160aade-2a82-43cd-b976-786d414f61a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ðŸš€ Feature extraction completed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
